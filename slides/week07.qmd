---
title: "Week 7: Regular Expressions and Web Scraping"
subtitle: "PM 566: Introduction to Health Data Science"
author: "George G. Vega Yon and Kelly Street"
format:
  revealjs:
    theme: default
    fig-width: 7
    fig-align: center
engine: knitr
---

# Today's goals

- Introduction to Regular Expressions

- Understand the fundamentals of Web Scraping

- Learn how to use an API

---

## Regular Expressions: What is it?

> A regular expression (shortened as regex or regexp; also referred to as rational expression) is a sequence of characters that define a search pattern. -- [Wikipedia](https://en.wikipedia.org/wiki/Regular_expression)

<img src="https://imgs.xkcd.com/comics/regular_expressions.png" width="450px">

---

## Regular Expressions: Why should you care?

We can use Regular Expressions for:

- Validating data fields, email address, numbers, etc.

- Searching text in various formats, e.g., addresses, there are many ways to write an address.

- Replace text, e.g., different spellings, `Storm`, `Stôrm`, `Stórm` to `Storm`.

- Remove text, e.g., tags from an HTML text, `<name>George</name>` to `George`.

---

## Regex 101: Metacharacters{.smaller}

What makes *regex* special is metacharacters. While we can always use *regex* to match literals like `dog`, `human`, `1999`, we only make use of all *regex* power when using metacharacters:

- `.` Any character except new line
- `^` beginning of the text
- `$` end of the text
- `[`expression`]` Match any single character in "expression", e.g.

    - `[0123456789]` Any digit
    - `[0-9]` Any digit in the range 0-9
    - `[a-z]` Lower-case letters
    - `[A-Z]` Upper-case letters
    - `[a-zA-Z]` Lower or upper case letters.
    - `[a-zA-Z0-9]` Any alpha-numeric

---

## Regex 101: Metacharacters (cont. 1){.smaller}

- `[^regex]` Match any except those in `regex`, e.g.

    - `[^0123456789]` Match any except a number
    - `[^0-9]` Match anything except in the range 0-9
    - `[^./ ]` any except dot, slash, and space.

---

## Regex 101: Metacharacters (cont. 2){.smaller}

Ranges, e.g., `0-9` or `a-z`, are locale- and implementation-dependent, meaning that the range of lower case letters may vary depending on the OS's language. To solve for this problem, you could use [Character classes](https://en.wikipedia.org/wiki/Regular_expression#Character_classes). Some examples:

- `[[:lower:]]` lower case letters in the current locale, could be `[a-z]`
- `[[:upper:]]` upper case letters in the current locale, could be `[A-Z]`
- `[[:alpha:]]` upper and lower case letters in the current locale, could be `[a-zA-Z]`
- `[[:digit:]]` Digits: 0 1 2 3 4 5 6 7 8 9
- `[[:alnum:]]` Alpha numeric characters `[[:alpha:]]` and `[[:digit:]]`.
- `[[:punct:]]` Punctuation characters: ! " \# $ % & ' ( ) * + , - . / : ; < = > ? @ [ \\ ] ^ _ &#96; \{ | \} ~.

For example, in the locale `en_US`, the word `Ḧóla` IS NOT fully matched by `[a-zA-Z]+`, but IT IS fully matched by `[[:alpha:]]+`.

---

## Regex 101: Metacharacters (cont. 3){.smaller}

Other important metacharacters:

- `\\s` white space, equivalent to `[\\r\\n\\t\\f\\v ]`
- `|` or (logical or).

---

## Regex 101: Metacharacters (cont. 4){.smaller}

These usually come together with specifying how many times (repetition):

- `regex?` Zero or one match.
- `regex*` Zero or more matches
- `regex+` One or more matches
- `regex{n,}` At least `n` matches
- `regex{,m}` at most `m` matches
- `regex{n,m}` Between `n` and `m` matches.

Where `regex` is a regular expression

---

## Regex 101: Metacharacters (cont. 5){.smaller}

There are other operators that can be very useful,

- `(regex)` Group capture.
- `(?:regex)` Group operation without capture.
- `A(?=B)` Look ahead (match). Find expression A where expression B follows. 
- `A(?!B)` Look ahead (don't match). Find expression A where expression B does not follow.
- `(?<=B)A` Look behind (match). Find expression A where expression B precedes.
- `(?<!B)A` Look behind (don't match). Find expression A where expression B does not precede.

Group captures can be reused with `\\1`, `\\2`, ..., `\\n`, referring to the first group, second group, etc.

More (great) information here [https://regex101.com/](https://regex101.com/)

---

## Regex 101: Examples{.smaller}

```{r setup-regex-examples, echo = FALSE}
library(stringr)
txt <- c("Hanna Perez [name]", "The 年 year was 1999", "HaHa, @abc said that", "GoGo trojans #2025!")
regex <- c(
  ".{5}",
  "n{2}",
  "[0-9]",
  "[0-9]+",
  "[a-zA-Z]+",
  "\\s[a-zA-Z]+\\s",
  "\\s[[:alpha:]]+\\s",
  "[a-zA-Z]+ [a-zA-Z]+", 
  "[a-zA-Z]+\\s?",
  "([a-zA-Z]+)\\1",
  "(@|#)[a-z0-9]+",
  "(?<=#|@)[a-z0-9]+",
  "\\[[a-z]+\\]"
  )
ans <- lapply(regex, stringr::str_extract, string = txt)
ans <- do.call(rbind, ans)
colnames(ans) <- txt
ans <- cbind(regex = regex, ans)
ans[is.na(ans)] <- ""
#ans[nrow(ans),1] <- "&#92&#91&#91a-z&#93+&#92&#93"
ans[,1] <- gsub('[\\]','\\\\\\\\', ans[,1]) # make backslashes print properly
```

1) ``r ans[1,1]`` Match **any character** (except line end) five times.

2) ``r ans[2,1]`` Match the letter **n** twice.

3) ``r ans[3,1]`` Match **any number** once.

4) ``r ans[4,1]`` Match **any number** at least once.

5) ``r ans[5,1]`` Match **any lower or upper case letter** at least once.

6) ``r ans[6,1]`` Match a **space**, **any lower or upper case letter** at least once, and a **space**. 

7) ``r ans[7,1]`` Same as before but this time with the `[[:alpha:]]` character class.

---

## Regex 101: Examples (cont. 1){.smaller}

Here we are extracting the **first occurrence** of the following regular expressions
(using `stringr::str_extract()`):

```{r regex-examples-extract, echo = FALSE}
knitr::kable(ans[1:7,], format = "html", escape = TRUE)
```

---

## Regex 101: Examples (cont. 3){.smaller}

8) ``r ans[8,1]`` Match two sets of letters separated by one space.

9) ``r ans[9,1]`` Match one set of letters, maybe followed by a white space.

10) ``r ans[10,1]`` Match **any lower or upper case letter** at least once and then match the same pattern again.

11) ``r ans[11,1]`` Match either the `@` or `#` symbol, followed by one or more **lower case letter** or **number**.

12) ``r ans[12,1]`` Match one or more **lower case letter** or **number** that follows either the `@` or `#` symbol.

13) ``r ans[13,1]`` Match the symbol `[`, at least one **lower case letter**, then symbol `]`.

---

## Regex 101: Examples (cont. 2){.smaller}

```{r regex-examples-extract2, echo = FALSE, results='asis'}
knitr::kable(ans[8:(nrow(ans)-1),], format = "html", escape = FALSE)
```

---

## Regex 101: Examples (cont. 3){.smaller}

```{r}
colnames(ans) <- c("regex", "Hanna Perez\n[name]", "The 年 year\nwas 1999", "HaHa, @abc\nsaid that", "GoGo trojans\n#2025!")
plot(c(0,50), c(0,8), asp=1, col='white', axes=FALSE, xlab='',ylab='')
for(i in 1:ncol(ans)){
    text(10*i-5, 7, colnames(ans)[i], adj=c(.5,.5), font=2)
}
text(5,1,"\\\\[[a-z]+\\\\]")
text(15,1,"[name]")
abline(h=3.5)
```

---

## Regex 101: Functions in R{.smaller}

1. Find in text: `grepl()`, `stringr::str_detect()`.

2. Similar to `which()`, which elements are `TRUE` `grep()`, `stringr::str_which()`

3. Replace the first instance: `sub()`, `stringr::str_replace()`

4. Replace all instances: `gsub()`, `stringr::str_replace_all()`

5. Extract text: `regmatches()` plus `regexpr()`, `stringr::str_extract()` and `stringr::str_extract_all()`.

![](https://stringr.tidyverse.org/logo.png)

---

## Regex 101: Functions in R (cont. 1){.smaller}

For example, like in Twitter, let's create a regex that matches usernames
or hashtags with the following pattern:

`(@|#)([[:alnum:]]+)`

```{r example-regex-2, echo = FALSE}
txt0 <- c("@Hanna Perez [name] #html", "The @年 year was 1999", "HaHa, @abc said that @z")
txt <- NULL
pattern <- "(@|#)([[:alnum:]]+)"
txt <- c(txt, str_detect(txt0, pattern))
txt <- c(txt, str_extract(txt0, pattern))
txt <- c(txt, paste0(sapply(str_extract_all(txt0, pattern), paste, collapse = ", ")))
txt <- c(txt, str_replace(txt0, pattern, "\\1justinbieber"))
txt <- c(txt, str_replace_all(txt0, pattern, "\\1justinbieber"))

txt <- matrix(txt, ncol = 3, byrow = TRUE)
colnames(txt) <- txt0
txt <- cbind(
  Code = c(
    "`str_detect(text, pattern)` or `grepl(pattern, text)`",
    "`str_extract(text, pattern)` or `regmatches(...)`",
    "`str_extract_all(text, pattern)`",
    "`str_replace(text, pattern, \"\\\\1justinbieber\")` or `sub(...)`",
    "`str_replace_all(text, pattern, \"\\\\1justinbieber\")` or `gsub(...)`"
  ), txt
)

knitr::kable(txt[1:3,])
```

---

## Regex 101: Functions in R (cont. 2){.smaller}

Pattern: `(@|#)([[:alnum:]]+)`

```{r example-regex-2-2, echo = FALSE}
knitr::kable(txt[4:5,])
```

**Note**: It renders oddly in the table, but there is no space or line break in the group replacement, `\\1`.

---

## Data

This week we will continue using our text mining dataset:

```{r load-data, echo=TRUE}
# Where are we getting the data from
mts_url <- "https://github.com/USCbiostats/data-science-data/raw/master/00_mtsamples/mtsamples.csv"

# Downloading the data to a tempfile (so it is destroyed afterwards)
tmp <- tempfile(pattern = "mtsamples", fileext = ".csv")

# We should be downloading this, ONLY IF this was not downloaded already.
# otherwise is just a waste of time.
if (!file.exists(tmp)) {
  download.file(
    url      = mts_url,
    destfile = tmp,
    # method   = "libcurl", timeout = 1000 (you may need this option)
  )
}

# read the file
mtsamples <- read.csv(tmp, header = TRUE, row.names = 1)
```


---

## Regex Lookup Text: Tumor{.smaller}

For each entry, we want to know if it is tumor-related. For that we can use
the following code:

```{r lookup-tumor, echo=TRUE}
# How many entries contain the word tumor?
sum(grepl("tumor", mtsamples$description, ignore.case = TRUE))

# Generating a column tagging tumor
mtsamples$tumor_related <- grepl("tumor", mtsamples$description, ignore.case = TRUE)

# Taking a look at a few examples
mtsamples$description[mtsamples$tumor_related == TRUE][1:3]
```

Notice the `ignore.case = TRUE`. This is equivalent to transforming the text to lower case using `tolower()` before passing the text to the regular expression function.

---

## Regex Lookup text: Pronoun of the patient

Now, let's try to guess the pronoun of the patient. To do so, we could tag by
using the words *he, his, him, they, them, theirs, ze, hir, hirs, she, hers, her* (see [this article on sexist text](https://dictionary.cambridge.org/grammar/british-grammar/sexist-language?q=He%2C+she%2C+him%2C+her%2C+his%2C+hers)):

```{r, echo=TRUE}
mtsamples$pronoun <- str_extract(
  string  = tolower(mtsamples$transcription),
  pattern = "he|his|him|they|them|theirs|ze|hir|hirs|she|hers|her"
)
```

What is the problem with this approach?

---

## Regex Lookup text: Pronoun of the patient (cont. 1){.smaller}

```{r, echo=TRUE}
mtsamples$transcription[1]
mtsamples$pronoun[1]
```

---

## Regex Lookup text: Pronoun of the patient (cont. 1){.smaller}

To correct this issue, we can make our regular expression more precise:

`(?<=\W|^)(he|his|him|they|them|theirs|ze|hir|hirs|she|hers|her)(?=\W|$)`

Bit by bit this is:

- `(?<=regex)` look behind for...
    - `\W` any non-alphanumeric character, this is equivalent to `[^[:alnum:]]`, `|` or
    - `^` the beginning of text
- `he|his|him...` any of these words
- `(?=regex)` followed by...
    - `\W` any non-alphanumeric character, this is equivalent to `[^[:alnum:]]`, `|` or
    - `$` the end of the text.

---

## Regex Lookup text: Pronoun of the patient (cont. 2){.smaller}

Let's use this new pattern:

```{r pronoun-regex, echo=TRUE}
mtsamples$pronoun <- str_extract(
  string  = tolower(mtsamples$transcription), 
  pattern = "(?<=\\W|^)(he|his|him|they|them|theirs|ze|hir|hirs|she|hers|her)(?=\\W|$)"
  )
mtsamples$pronoun[1:10]
```

---

## Regex Lookup text: Pronoun of the patient (cont. 3)

```{r Pronoun-regex-cont, echo=TRUE}
table(mtsamples$pronoun, useNA = "always")
```


---

## Regex Extract Text: Type of Cancer

- Imagine now that you need to see the types of cancer mentioned in the data.

- For simplicity, let's assume that, if specified, it is in the form of `TYPE cancer`, i.e. single word.

- We are interested in the word before cancer, how can we capture this?

---

## Regex Extract Text: Type of Cancer (cont 1.){.smaller}

We can just try to **extract** the phrase `"[some word] cancer"`, in particular, we could use the
following regular expression:

`[[:alnum:]-_]{4,}\s*cancer`

Where

- `[[:alnum:]-_]{4,}` captures any alphanumeric character, including `-` and `_`. 
   Furthermore, for this match to work there must be at least 4 characters,
- `\s*` captures 0 or more white-spaces, and
- `cancer` captures the word cancer

---

## Regex Extract Text: Type of Cancer (cont. 2){.smaller}

```{r cancer-regex, echo=TRUE}
mtsamples$cancer_type <- str_extract(tolower(mtsamples$keywords), "[[:alnum:]-_]{4,}\\s*cancer")
table(mtsamples$cancer_type)
```

---

# Web Scraping

---

## Fundamentals of Web Scraping{.smaller}

**What?**

> Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites -- [Wikipedia](https://en.wikipedia.org/wiki/Web_scraping)

**How?**

- The [`rvest`](https://cran.r-project.org/package=rvest) R package provides various tools for reading and processing web data.

- Under the hood, `rvest` is a wrapper of the [`xml2`](https://cran.r-project.org/package=xml2)
and [`httr`](https://cran.r-project.org/package=httr) R packages.

(in the case of [dynamic websites](https://en.wikipedia.org/wiki/Dynamic_web_page), take a look at [selenium](https://en.wikipedia.org/wiki/Selenium_(software)))

---

## Web scraping raw HTML: Example{.smaller}

We would like to capture the table of [COVID-19 death rates](https://en.wikipedia.org/wiki/COVID-19_pandemic_death_rates_by_country) per country directly from Wikipedia.

```{r setup-scrape, echo=TRUE}
library(rvest)
library(xml2)

# Reading the HTML table with the function xml2::read_html
covid <- read_html(
  x = "https://en.wikipedia.org/wiki/COVID-19_pandemic_death_rates_by_country"
  )

# Let's look at the the output
covid
```

---

## Web scraping raw HTML: Example (cont. 1){.smaller}

- We want to get the HTML table that shows up in the doc. To do so, we can use the
  function `xml2::xml_find_all()` and `rvest::html_table()`

- The first will locate the place in the document that matches a given **XPath**
  expression.
  
- [XPath](https://en.wikipedia.org/wiki/XPath), XML Path Language, is a query language to select nodes in a XML
  document.
  
- A nice tutorial can be found [here](https://www.w3schools.com/xml/xpath_intro.asp)

- Modern Web browsers make it easy to use XPath!

Live Example! (inspect elements in [Google Chrome](https://developers.google.com/web/tools/chrome-devtools/open),
[Mozilla Firefox](https://developer.mozilla.org/en-US/docs/Tools/Page_Inspector/How_to/Open_the_Inspector), [Internet Explorer](https://docs.microsoft.com/en-us/microsoft-edge/devtools-guide-chromium/ie-mode), and [Safari](https://developer.apple.com/library/archive/documentation/NetworkingInternetWeb/Conceptual/Web_Inspector_Tutorial/EditingCode/EditingCode.html#//apple_ref/doc/uid/TP40017576-CH4-DontLinkElementID_25))
  
---

## Web scraping with `xml2` and the `rvest` package (cont. 2)

Now that we know the path, let's use that and extract the table:

```{r get-table-covid1, echo=TRUE}
table <- xml_find_all(covid, xpath = '/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/div[4]/table')
print(table) # collection of tables in XML
```

---

## Web scraping with `xml2` and the `rvest` package (cont. 3)

We're getting closer!

```{r get-table-covid2, echo=TRUE}
table <- html_table(table) 
print(table) # list of tibbles
```

---

## Web scraping with `xml2` and the `rvest` package (cont. 4)

There it is!

```{r get-table-covid3, echo=TRUE}
table <- table[[1]]
print(table) # hooray!
```


---

## * Web APIs{.smaller}

**What?**

> A Web API is an application programming interface for either a web server or a web browser. -- [Wikipedia](https://en.wikipedia.org/wiki/Web_API)

Some examples include: [twitter API](https://developer.twitter.com/en), [facebook API](https://developers.facebook.com/), [Gene Ontology API](http://api.geneontology.org/api)

**How?**

You can request data, the **GET method**, post data, the **POST method**, and do many other things using the [HTTP protocol](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol).

**How in R?**

For this part, we will be using the `httr()` package, which is a wrapper of the
`curl()` package, which in turn provides access to the `curl` library that
is used to communicate with APIs.

---

## * Web APIs with curl

<div align="center">
<img src="https://cdn.tutsplus.com/net/authors/jeremymcpeak/http1-url-structure.png" width="700px">
<br>
Structure of a URL (source: <a href="https://code.tutsplus.com/tutorials/http-the-protocol-every-web-developer-must-know-part-1--net-31177" target="_blank">"HTTP: The Protocol Every Web Developer Must Know - Part 1"</a>)
</div>

---

## * Web APIs with curl

Under the hood, the `httr` (and thus `curl`) sends request somewhat like this

```bash
curl -X GET https://google.com -w "%{content_type}\n%{http_code}\n"
```

A get request (`-X GET`) to `https://google.com`, which also includes (`-w`) the following:
`content_type` and `http_code`:

```html
<HTML><HEAD><meta http-equiv="content-type" content="text/html;charset=utf-8">
<TITLE>301 Moved</TITLE></HEAD><BODY>
<H1>301 Moved</H1>
The document has moved
<A HREF="https://www.google.com/">here</A>.
</BODY></HTML>
text/html; charset=UTF-8
301
```

We use the `httr` R package to make life easier.

---

## * Web API Example 1: Gene Ontology{.smaller}

- We will make use of the [Gene Ontology API](http://api.geneontology.org/api).

- We want to know what genes (human or not) are **involved in** the function **antiviral innate immune response** (go term [GO:0140374](http://amigo.geneontology.org/amigo/term/GO:0140374)), looking only at those annotations that have evidence code [ECO:0000006](https://evidenceontology.org/browse/#ECO_0000006) (experimental evidence):

```{r example-gene-ontology, cache=TRUE, echo=TRUE}
library(httr)
go_query <- GET(
  url   = "http://api.geneontology.org/",
  path  = "api/bioentity/function/GO:0140374/genes",
  query = list(
    evidence          = "ECO:0000006",
    relationship_type = "involved_in"
  ), 
  # May need to pass this option to curl to allow to wait for at least
  # 60 seconds before returning error.
  config = config(
    connecttimeout = 60
    )
)
```

We could have also passed the full URL directly...

---

## * Web API Example 1: Gene Ontology (cont. 1)

Let's take a look at the curl call:

```bash
curl -X GET "http://api.geneontology.org/api/bioentity/function/GO:0140374/genes?evidence=ECO%3A0000006&relationship_type=involved_in" -H "accept: application/json"
```

What `httr::GET()` does:

```r
> go_query$request
## <request>
## GET http://api.geneontology.org/api/bioentity/function/GO:0140374/genes?evidence=ECO%3A0000006&relationship_type=involved_in
## Output: write_memory
## Options:
## * useragent: libcurl/7.58.0 r-curl/4.3 httr/1.4.1
## * connecttimeout: 60
## * httpget: TRUE
## Headers:
## * Accept: application/json, text/xml, application/xml, */*
```

---

## * Web API Example 1: Gene Ontology (cont. 2){.smaller}

Let's take a look at the response:

```{r example-gene-ontology-cont, cache=TRUE, echo=TRUE}
go_query
```

Remember the codes:

 - 1xx: Information message
 - 2xx: Success
 - 3xx: Redirection
 - 4xx: Client error
 - 5xx: Server error
 
---

## * Web API Example 1: Gene Ontology (cont. 3){.smaller}

We can extract the results using the `httr::content()` function:

```{r echo=FALSE}
library(httr)
```


```{r get-contents-go, dependson="example-gene-ontology", echo=TRUE}
dat <- content(go_query) 
dat <- lapply(dat$associations, function(a) {
  data.frame(
    Gene        = a$subject$id,
    taxon_id    = a$subject$taxon$id,
    taxon_label = a$subject$taxon$label
  )
})
dat <- do.call(rbind, dat)
str(dat)
```


---

## * Web API Example 1: Gene Ontology (cont. 4){.smaller}

The structure of the result will depend on the API. In this case, the output was a JSON file, so the content function returns a list in R. In other scenarios it could return an XML object.

```{r get-contents-go-print, echo=TRUE}
knitr::kable(head(dat),
  caption = "Genes experimentally annotated with the function\
  **antiviral innate immune response** (GO:0140374)"
  )
```

---

## * Web API Example 2: Using Tokens

- Sometimes, APIs are not completely open, you need to register.

- The API may require to login (user+password), or pass a token.

- In this example, I'm using a token which I obtained [here](https://www.ncdc.noaa.gov/cdo-web/token)

- You can find information about the [National Centers for Environmental Information](https://www.ncdc.noaa.gov/)
  API [here](https://www.ncdc.noaa.gov/cdo-web/webservices/v2)

---

## * Web API Example 2: Using Tokens (cont. 1)

- The way to pass the token will depend on the API service.

- Some require authentication, others need you to pass it as an argument of the query,
  i.e., directly in the URL.
  
- In this case, we pass it on the header.
  
  ```{r donwload-climate-data, eval = FALSE}
  stations_api <- GET(
    url    = "https://www.ncdc.noaa.gov",
    path   = "cdo-web/api/v2/stations",
    config = add_headers(
      token = "[YOUR TOKEN HERE]"
      ),
    query  = list(limit = 1000)
  )
  ```
  
  
  This is equivalent to using the following query
  
  ```bash
  curl --header "token: [YOUR TOKEN HERE]" \
    https://www.ncdc.noaa.gov/cdo-web/api/v2/stations?limit=1000
  ```

**Note**: This won't run, you need to get your own token

---

## * Web API Example 2: Using Tokens (cont. 2){.smaller}

Again, we can recover the data using the `content()` function:

```{r download-climate-data-cont, eval = FALSE, echo=TRUE}
ans <- content(stations_api)
ans$results[[1]]
## $elevation
## [1] 139
## 
## $mindate
## [1] "1948-01-01"
## 
## $maxdate
## [1] "2014-01-01"
## 
## $latitude
## [1] 31.5702
## 
## $name
## [1] "ABBEVILLE, AL US"
## 
## $datacoverage
## [1] 0.8813
## 
## $id
## [1] "COOP:010008"
```

---

## * Web API Example 3: HHS health recommendation{.smaller}

Here is a last example. We will use the Department of Health and Human Services
API for "[...] demographic-specific health recommendations" (details at [health.gov](https://health.gov/our-work/health-literacy/consumer-health-content/free-web-content/apis-developers/documentation))

```{r health-advice, cache=TRUE, echo=TRUE}
health_advises <- GET(
  url  = "https://health.gov/", 
  path = "myhealthfinder/api/v3/myhealthfinder.json",
  query = list(
    lang = "en",
    age  = "32",
    sex  = "male",
    tobaccoUse = 0
  ),
  config = c(
    add_headers(accept = "application/json"),
    config(connecttimeout = 60)
  )
)
```

---

## * Web API Example 3: HHS health recommendation (cont. 1){.smaller}

Let's see the response

```{r health-advice-response, echo=TRUE}
health_advises
```


---

## * Web API Example 3: HHS health recommendation (cont. 2){.smaller}

```{r health-advice-print, results='asis', echo=TRUE}
# Extracting the content
health_advises_ans <- content(health_advises)

# Getting the titles
txt <- with(health_advises_ans$Result$Resources, c(
  sapply(all$Resource, "[[", "Title"),
  sapply(some$Resource, "[[", "Title"),
  sapply(`You may also be interested in these health topics:`$Resource, "[[", "Title")
))
cat(txt, sep = "; ")
```

---

## Summary{.smaller}

- We learned about regular expressions with the package **stringr** (a wrapper of **stringi**)

- We can use regular expressions to detect (`str_detect()`), replace (`str_replace()`), and 
extract (`str_extract()`) expressions.

- We looked at web scraping using the **rvest** package (a wrapper of **xml2**).

- We extracted elements from the HTML/XML using `xml_find_all()` with XPath expressions.

- We also used the `html_table()` function from rvest to extract tables from HTML documents.

- We took a quick review on Web APIs and the Hyper-text-transfer-protocol (HTTP).

- We used the **httr** R package (wrapper of **curl**) to make `GET` requests to various APIs

- We even showed an example using a token passed via the `header`.

- Once we got the responses, we used the `content()` function to extract the message of the
response.

---

## Detour on CURL options

Sometimes you will need to change the default set of options in CURL. You can
checkout the list of options in `curl::curl_options()`. A common hack is to 
extend the time-limit before dropping the conection, e.g.:

Using the **Health IT** API from the US government, we can obtain the
**Electronic Prescribing Adoption and Use by County** (see docs
[here](https://dashboard.healthit.gov/datadashboard/documentation/electronic-prescribing-adoption-use-data-documentation-county.php))

The problem is that it usually takes longer to get the data, so we pass 
the config option `connecttimeout` (which corresponds to the flag `--connect-timeout`)
in the curl call (see next slide)

---

## Detour on CURL options (cont. 1){.smaller}

```{r eval = FALSE, echo=TRUE}
ans <- httr::GET(
  url    = "https://dashboard.healthit.gov/api/open-api.php",
  query  = list(
    source = "AHA_2008-2015.csv",
    region = "California",
    period = 2015
    ),
  config = config(
    connecttimeout = 60
    )
)
```

```r
> ans$request
# <request>
# GET https://dashboard.healthit.gov/api/open-api.php?source=AHA_2008-2015.csv&region=California&period=2015
# Output: write_memory
# Options:
# * useragent: libcurl/7.58.0 r-curl/4.3 httr/1.4.1
# * connecttimeout: 60
# * httpget: TRUE
# Headers:
# * Accept: application/json, text/xml, application/xml, */*
```

---

## Regular Expressions: Email validation

This is the official regex for email validation implemented by [RCF 5322](http://www.ietf.org/rfc/rfc5322.txt) 

```
(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|"(?:[\x01-\x08
\x0b\x0c\x0e-\x1f\x21\x23-\x5b\x5d-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])*")@(?:(?
:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\[(?:(?:(2(5[
0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0
-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21-\x5a\x53-\
x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])+)\])
```

See the corresponding post in [StackOverflow](https://stackoverflow.com/a/201378/2097171) 

